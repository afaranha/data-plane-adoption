= Migrating Ceph Manager daemons to {Ceph} nodes

You must migrate your Ceph Manager daemons from the {rhos_prev_long} ({OpenStackShort}) Controller nodes to a set of target nodes. Target nodes are either existing {Ceph} nodes, or {OpenStackShort} Compute nodes if {Ceph} is deployed by {OpenStackPreviousInstaller} with a Hyperconverged Infrastructure (HCI) topology.

[NOTE]
The following procedure uses `cephadm` and the Ceph Orchestrator to drive the Ceph Manager migration, and the Ceph spec to modify the placement and reschedule the Ceph Manager daemons. Ceph Manager is run in an active/passive state. It also provides many modules, including the Ceph Orchestrator. Every potential module, such as the Ceph Dashboard, that is provided by `ceph-mgr` is implicitly migrated with Ceph Manager.

.Prerequisites

* The target nodes, CephStorage or ComputeHCI, are configured to have both `storage` and `storage_mgmt` networks. This ensures that you can use both {Ceph} public and cluster networks from the same node.
+
[NOTE]
This step requires you to interact with {OpenStackPreviousInstaller}. From {OpenStackShort} {rhos_prev_ver} and later you do not have to run a stack update.

.Procedure

. SSH into the target node and enable the firewall rules that are required to reach a Ceph Manager service:
+
----
dports="6800:7300"
ssh heat-admin@<target_node> sudo iptables -I INPUT \
    -p tcp --match multiport --dports $dports -j ACCEPT;
----
+
* Replace `<target_node>` with the hostname of the hosts that are listed in the {Ceph} environment. Run `ceph orch host ls` to see the list of the hosts.
+
Repeat this step for each target node.

. Check that the rules are properly applied to the target node and persist them:
+
----
$ sudo iptables-save
$ sudo systemctl restart iptables
----
+
. Prepare the target node to host the new Ceph Manager daemon, and add the `mgr`
label to the target node:
+
----
$ ceph orch host label add <target_node> mgr; done
----

. Repeat steps 1-3 for each target node that hosts a Ceph Manager daemon.

. Get the Ceph Manager spec:
+
[source,yaml]
----
$ sudo cephadm shell -- ceph orch ls --export mgr > mgr.yaml
----

. Edit the retrieved spec and add the `label: mgr` section to the `placement`
section:
+
[source,yaml]
----
service_type: mgr
service_id: mgr
placement:
  label: mgr
----

. Save the spec in the `/tmp/mgr.yaml` file.
. Apply the spec with `cephadm`` by using the Ceph Orchestrator:
+
----
$ sudo cephadm shell -m /tmp/mgr.yaml -- ceph orch apply -i /mnt/mgr.yaml
----

.Verification

. Verify that the new Ceph Manager daemons are created in the target nodes:
+
----
$ ceph orch ps | grep -i mgr
$ ceph -s
----
+
The Ceph Manager daemon count should match the number of hosts where the `mgr` label is added.
+
[NOTE]
The migration does not shrink the Ceph Manager daemons. The count grows by
the number of target nodes, and migrating Ceph Monitor daemons to {Ceph} nodes
decommissions the stand-by Ceph Manager instances. For more information, see
xref:migrating-mon-from-controller-nodes_migrating-ceph-rbd[Migrating Ceph Monitor daemons to {Ceph} nodes].
